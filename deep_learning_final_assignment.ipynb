{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final assignment\n",
    "\n",
    "<br/> Martynas Motiejunas\n",
    "<br/> email: martynas.mo8777@go.kauko.lt\n",
    "\n",
    "## Task:\n",
    "\n",
    "Download 10 years of data from website: https://finance.yahoo.com/ company of choice and build a neural networks to predict it's stock price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2009-07-30</td>\n",
       "      <td>19.570000</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>19.330000</td>\n",
       "      <td>14.056325</td>\n",
       "      <td>60640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2009-07-31</td>\n",
       "      <td>19.340000</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>13.998153</td>\n",
       "      <td>47557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2009-08-03</td>\n",
       "      <td>19.480000</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>19.180000</td>\n",
       "      <td>19.370001</td>\n",
       "      <td>14.085413</td>\n",
       "      <td>44330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2009-08-04</td>\n",
       "      <td>19.209999</td>\n",
       "      <td>19.379999</td>\n",
       "      <td>19.150000</td>\n",
       "      <td>19.320000</td>\n",
       "      <td>14.049059</td>\n",
       "      <td>46880800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2009-08-05</td>\n",
       "      <td>19.270000</td>\n",
       "      <td>19.299999</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>13.814657</td>\n",
       "      <td>60035000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2009-07-30  19.570000  19.799999  19.299999  19.330000  14.056325  60640900\n",
       "1  2009-07-31  19.340000  19.559999  19.250000  19.250000  13.998153  47557600\n",
       "2  2009-08-03  19.480000  19.559999  19.180000  19.370001  14.085413  44330800\n",
       "3  2009-08-04  19.209999  19.379999  19.150000  19.320000  14.049059  46880800\n",
       "4  2009-08-05  19.270000  19.299999  18.860001  18.860001  13.814657  60035000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the dataset and loading the file contents \n",
    "dataset_train = pd.read_csv('INTEL_train.csv')\n",
    "#calling head() function to see if the file is read\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19.57    ],\n",
       "       [19.34    ],\n",
       "       [19.48    ],\n",
       "       ...,\n",
       "       [52.689999],\n",
       "       [51.529999],\n",
       "       [51.709999]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperating \"Open\" column data because we are going to use it to train our neural network\n",
    "training_set = dataset_train.iloc[:, 1:2].values \n",
    "# Show separated \"Open\" colum data in an array\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1 using MinMaxScaler. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# importing the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a scaler instance to rescale all data to the range of 0 to 1\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04120949],\n",
       "       [0.0356011 ],\n",
       "       [0.0390149 ],\n",
       "       ...,\n",
       "       [0.84881736],\n",
       "       [0.82053158],\n",
       "       [0.82492075]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "#Then checking it with this command:\n",
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data structure\n",
    "\n",
    "#The 90 stock prices in the last 4.5 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 90 because that is the first instance allowing us to \n",
    "# go back 90 days\n",
    "for i in range(90, 2517): \n",
    "    # 0 is the column ID, the only column in this case.    \n",
    "    # put the last 90 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04120949, 0.0356011 , 0.0390149 , ..., 0.04437947, 0.0458425 ,\n",
       "        0.05632777],\n",
       "       [0.0356011 , 0.0390149 , 0.03243111, ..., 0.0458425 , 0.05632777,\n",
       "        0.05681541],\n",
       "       [0.0390149 , 0.03243111, 0.0338942 , ..., 0.05632777, 0.05681541,\n",
       "        0.05779081],\n",
       "       ...,\n",
       "       [0.89222146, 0.8822239 , 0.87881002, ..., 0.83199222, 0.83150454,\n",
       "        0.84442816],\n",
       "       [0.8822239 , 0.87881002, 0.89051458, ..., 0.83150454, 0.84442816,\n",
       "        0.84881736],\n",
       "       [0.87881002, 0.89051458, 0.85564501, ..., 0.84442816, 0.84881736,\n",
       "        0.82053158]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the X and Y arrays into NumPy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the Matrix\n",
    "\n",
    "We need to add a new matrix dimension to accommodate the indicator (predictor). \n",
    "\n",
    "NumPy matrices are tensors (3D) and essentially we need to specify that our matrix consists of **90 days** (dimension x) times **total days in data set** (dimension y) times **1 value per matrix cell (scalar)** (dimension z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding First Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the LSTM (Long short-term memory) layer\n",
    "regressor.add(LSTM(units = 70, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%)\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 70, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 70, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 70))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Output Layer & Compiling\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 0.0146\n",
      "Epoch 2/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0028\n",
      "Epoch 3/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0028\n",
      "Epoch 4/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0025\n",
      "Epoch 5/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0024\n",
      "Epoch 6/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0023\n",
      "Epoch 7/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0023\n",
      "Epoch 8/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0023\n",
      "Epoch 9/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0019\n",
      "Epoch 10/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0019\n",
      "Epoch 11/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0019\n",
      "Epoch 12/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0017\n",
      "Epoch 13/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0015\n",
      "Epoch 14/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0016\n",
      "Epoch 15/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0018\n",
      "Epoch 16/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0015\n",
      "Epoch 17/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0014\n",
      "Epoch 18/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0013\n",
      "Epoch 19/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0015\n",
      "Epoch 20/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0013\n",
      "Epoch 21/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0013\n",
      "Epoch 22/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0013\n",
      "Epoch 23/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 0.0014\n",
      "Epoch 24/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0013\n",
      "Epoch 25/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0014\n",
      "Epoch 26/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0013\n",
      "Epoch 27/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0011\n",
      "Epoch 28/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0013\n",
      "Epoch 29/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0011\n",
      "Epoch 30/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0012\n",
      "Epoch 31/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0013\n",
      "Epoch 32/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0011\n",
      "Epoch 33/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0011\n",
      "Epoch 34/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0011\n",
      "Epoch 35/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 0.0010\n",
      "Epoch 36/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 8.9001e-04\n",
      "Epoch 37/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 9.0175e-04\n",
      "Epoch 38/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 9.1214e-04\n",
      "Epoch 39/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.8967e-04\n",
      "Epoch 40/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 9.8628e-04\n",
      "Epoch 41/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 9.5804e-04\n",
      "Epoch 42/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.8211e-04\n",
      "Epoch 43/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.8266e-04\n",
      "Epoch 44/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 9.0922e-04\n",
      "Epoch 45/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.3285e-04\n",
      "Epoch 46/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 7.7093e-04\n",
      "Epoch 47/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.6079e-04\n",
      "Epoch 48/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 9.6134e-04\n",
      "Epoch 49/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.8892e-04\n",
      "Epoch 50/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 8.0147e-04\n",
      "Epoch 51/100\n",
      "2427/2427 [==============================] - 16s 6ms/step - loss: 8.1667e-04\n",
      "Epoch 52/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 8.8006e-04\n",
      "Epoch 53/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 7.6741e-04\n",
      "Epoch 54/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.4603e-04\n",
      "Epoch 55/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 7.9054e-04\n",
      "Epoch 56/100\n",
      "2427/2427 [==============================] - 16s 6ms/step - loss: 8.6469e-04\n",
      "Epoch 57/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 7.5289e-04\n",
      "Epoch 58/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 8.6766e-04\n",
      "Epoch 59/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 7.1308e-04\n",
      "Epoch 60/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.8946e-04\n",
      "Epoch 61/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 8.1263e-04\n",
      "Epoch 62/100\n",
      "2427/2427 [==============================] - 16s 6ms/step - loss: 7.6934e-04\n",
      "Epoch 63/100\n",
      "2427/2427 [==============================] - 16s 7ms/step - loss: 7.1636e-04\n",
      "Epoch 64/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 7.2612e-04\n",
      "Epoch 65/100\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 7.4944e-04\n",
      "Epoch 66/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 6.9604e-04\n",
      "Epoch 67/100\n",
      "2427/2427 [==============================] - 14s 6ms/step - loss: 7.1852e-04\n",
      "Epoch 68/100\n",
      "2427/2427 [==============================] - 16s 7ms/step - loss: 7.1976e-04\n",
      "Epoch 69/100\n",
      "2427/2427 [==============================] - 15s 6ms/step - loss: 7.1827e-04\n",
      "Epoch 70/100\n",
      "2427/2427 [==============================] - 16s 6ms/step - loss: 7.0496e-04\n",
      "Epoch 71/100\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 7.2505e-04\n",
      "Epoch 72/100\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 8.2833e-04\n",
      "Epoch 73/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 7.5840e-04\n",
      "Epoch 74/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.4357e-04\n",
      "Epoch 75/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 7.0744e-04\n",
      "Epoch 76/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 9.3085e-04\n",
      "Epoch 77/100\n",
      "2427/2427 [==============================] - 18s 8ms/step - loss: 6.0128e-04\n",
      "Epoch 78/100\n",
      "2427/2427 [==============================] - 18s 8ms/step - loss: 6.4488e-04\n",
      "Epoch 79/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.4338e-04\n",
      "Epoch 80/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.5169e-04\n",
      "Epoch 81/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 7.0165e-04\n",
      "Epoch 82/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 7.0947e-04\n",
      "Epoch 83/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 7.4580e-04\n",
      "Epoch 84/100\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 6.4375e-04\n",
      "Epoch 85/100\n",
      "2427/2427 [==============================] - 21s 9ms/step - loss: 6.6152e-04\n",
      "Epoch 86/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.3287e-04\n",
      "Epoch 87/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 6.6052e-04\n",
      "Epoch 88/100\n",
      "2427/2427 [==============================] - 19s 8ms/step - loss: 6.1827e-04\n",
      "Epoch 89/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 7.4426e-04\n",
      "Epoch 90/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.1083e-04\n",
      "Epoch 91/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.2171e-04\n",
      "Epoch 92/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.7024e-04\n",
      "Epoch 93/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.6338e-04\n",
      "Epoch 94/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.2353e-04\n",
      "Epoch 95/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.8529e-04\n",
      "Epoch 96/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.3264e-04\n",
      "Epoch 97/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 6.7623e-04\n",
      "Epoch 98/100\n",
      "2427/2427 [==============================] - 17s 7ms/step - loss: 6.3252e-04\n",
      "Epoch 99/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 6.9561e-04\n",
      "Epoch 100/100\n",
      "2427/2427 [==============================] - 18s 7ms/step - loss: 6.1416e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f542cf504e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>51.509998</td>\n",
       "      <td>51.720001</td>\n",
       "      <td>50.049999</td>\n",
       "      <td>50.549999</td>\n",
       "      <td>50.210991</td>\n",
       "      <td>24169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>50.520000</td>\n",
       "      <td>51.889999</td>\n",
       "      <td>49.470001</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>49.168034</td>\n",
       "      <td>34020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>49.090000</td>\n",
       "      <td>49.360001</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>48.680000</td>\n",
       "      <td>48.353531</td>\n",
       "      <td>27881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-08-05</td>\n",
       "      <td>47.759998</td>\n",
       "      <td>47.959999</td>\n",
       "      <td>46.570000</td>\n",
       "      <td>46.970001</td>\n",
       "      <td>46.655003</td>\n",
       "      <td>38936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-08-06</td>\n",
       "      <td>47.150002</td>\n",
       "      <td>47.560001</td>\n",
       "      <td>46.770000</td>\n",
       "      <td>46.959999</td>\n",
       "      <td>46.959999</td>\n",
       "      <td>26119600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>46.299999</td>\n",
       "      <td>46.880001</td>\n",
       "      <td>45.970001</td>\n",
       "      <td>46.730000</td>\n",
       "      <td>46.730000</td>\n",
       "      <td>29440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2019-08-08</td>\n",
       "      <td>46.160000</td>\n",
       "      <td>47.369999</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>47.169998</td>\n",
       "      <td>47.169998</td>\n",
       "      <td>30643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>46.939999</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>45.779999</td>\n",
       "      <td>45.980000</td>\n",
       "      <td>45.980000</td>\n",
       "      <td>24975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>45.759998</td>\n",
       "      <td>46.070000</td>\n",
       "      <td>45.439999</td>\n",
       "      <td>45.599998</td>\n",
       "      <td>45.599998</td>\n",
       "      <td>18490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2019-08-13</td>\n",
       "      <td>45.490002</td>\n",
       "      <td>47.049999</td>\n",
       "      <td>45.360001</td>\n",
       "      <td>46.840000</td>\n",
       "      <td>46.840000</td>\n",
       "      <td>28959500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>46.060001</td>\n",
       "      <td>46.630001</td>\n",
       "      <td>45.650002</td>\n",
       "      <td>45.869999</td>\n",
       "      <td>45.869999</td>\n",
       "      <td>25650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2019-08-15</td>\n",
       "      <td>46.099998</td>\n",
       "      <td>46.180000</td>\n",
       "      <td>45.439999</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>21896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2019-08-16</td>\n",
       "      <td>46.340000</td>\n",
       "      <td>46.680000</td>\n",
       "      <td>46.060001</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>46.500000</td>\n",
       "      <td>22701800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2019-08-19</td>\n",
       "      <td>47.459999</td>\n",
       "      <td>47.599998</td>\n",
       "      <td>47.040001</td>\n",
       "      <td>47.230000</td>\n",
       "      <td>47.230000</td>\n",
       "      <td>21403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2019-08-20</td>\n",
       "      <td>47.029999</td>\n",
       "      <td>47.119999</td>\n",
       "      <td>46.459999</td>\n",
       "      <td>46.599998</td>\n",
       "      <td>46.599998</td>\n",
       "      <td>23113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2019-08-21</td>\n",
       "      <td>47.110001</td>\n",
       "      <td>47.290001</td>\n",
       "      <td>46.889999</td>\n",
       "      <td>47.150002</td>\n",
       "      <td>47.150002</td>\n",
       "      <td>15915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2019-08-22</td>\n",
       "      <td>47.279999</td>\n",
       "      <td>47.430000</td>\n",
       "      <td>46.689999</td>\n",
       "      <td>46.779999</td>\n",
       "      <td>46.779999</td>\n",
       "      <td>19783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2019-08-23</td>\n",
       "      <td>46.349998</td>\n",
       "      <td>46.630001</td>\n",
       "      <td>44.799999</td>\n",
       "      <td>44.959999</td>\n",
       "      <td>44.959999</td>\n",
       "      <td>32814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>45.820000</td>\n",
       "      <td>45.820000</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.560001</td>\n",
       "      <td>45.560001</td>\n",
       "      <td>22080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2019-08-27</td>\n",
       "      <td>45.869999</td>\n",
       "      <td>46.119999</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>16925500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>45.910000</td>\n",
       "      <td>45.369999</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>45.790001</td>\n",
       "      <td>14888800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2019-08-29</td>\n",
       "      <td>46.459999</td>\n",
       "      <td>47.220001</td>\n",
       "      <td>46.400002</td>\n",
       "      <td>46.869999</td>\n",
       "      <td>46.869999</td>\n",
       "      <td>17803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2019-08-30</td>\n",
       "      <td>47.240002</td>\n",
       "      <td>47.790001</td>\n",
       "      <td>47.160000</td>\n",
       "      <td>47.410000</td>\n",
       "      <td>47.410000</td>\n",
       "      <td>16922600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Open       High        Low      Close  Adj Close  \\\n",
       "0   2019-07-31  51.509998  51.720001  50.049999  50.549999  50.210991   \n",
       "1   2019-08-01  50.520000  51.889999  49.470001  49.500000  49.168034   \n",
       "2   2019-08-02  49.090000  49.360001  48.500000  48.680000  48.353531   \n",
       "3   2019-08-05  47.759998  47.959999  46.570000  46.970001  46.655003   \n",
       "4   2019-08-06  47.150002  47.560001  46.770000  46.959999  46.959999   \n",
       "5   2019-08-07  46.299999  46.880001  45.970001  46.730000  46.730000   \n",
       "6   2019-08-08  46.160000  47.369999  45.910000  47.169998  47.169998   \n",
       "7   2019-08-09  46.939999  47.000000  45.779999  45.980000  45.980000   \n",
       "8   2019-08-12  45.759998  46.070000  45.439999  45.599998  45.599998   \n",
       "9   2019-08-13  45.490002  47.049999  45.360001  46.840000  46.840000   \n",
       "10  2019-08-14  46.060001  46.630001  45.650002  45.869999  45.869999   \n",
       "11  2019-08-15  46.099998  46.180000  45.439999  45.700001  45.700001   \n",
       "12  2019-08-16  46.340000  46.680000  46.060001  46.500000  46.500000   \n",
       "13  2019-08-19  47.459999  47.599998  47.040001  47.230000  47.230000   \n",
       "14  2019-08-20  47.029999  47.119999  46.459999  46.599998  46.599998   \n",
       "15  2019-08-21  47.110001  47.290001  46.889999  47.150002  47.150002   \n",
       "16  2019-08-22  47.279999  47.430000  46.689999  46.779999  46.779999   \n",
       "17  2019-08-23  46.349998  46.630001  44.799999  44.959999  44.959999   \n",
       "18  2019-08-26  45.820000  45.820000  45.250000  45.560001  45.560001   \n",
       "19  2019-08-27  45.869999  46.119999  45.500000  45.790001  45.790001   \n",
       "20  2019-08-28  45.700001  45.910000  45.369999  45.790001  45.790001   \n",
       "21  2019-08-29  46.459999  47.220001  46.400002  46.869999  46.869999   \n",
       "22  2019-08-30  47.240002  47.790001  47.160000  47.410000  47.410000   \n",
       "\n",
       "      Volume  \n",
       "0   24169800  \n",
       "1   34020800  \n",
       "2   27881600  \n",
       "3   38936600  \n",
       "4   26119600  \n",
       "5   29440400  \n",
       "6   30643700  \n",
       "7   24975500  \n",
       "8   18490300  \n",
       "9   28959500  \n",
       "10  25650200  \n",
       "11  21896000  \n",
       "12  22701800  \n",
       "13  21403600  \n",
       "14  23113300  \n",
       "15  15915000  \n",
       "16  19783000  \n",
       "17  32814000  \n",
       "18  22080500  \n",
       "19  16925500  \n",
       "20  14888800  \n",
       "21  17803800  \n",
       "22  16922600  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price data file for testing our RNN\n",
    "\n",
    "dataset_test = pd.read_csv('INTEL_test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51.509998],\n",
       "       [50.52    ],\n",
       "       [49.09    ],\n",
       "       [47.759998],\n",
       "       [47.150002],\n",
       "       [46.299999],\n",
       "       [46.16    ],\n",
       "       [46.939999],\n",
       "       [45.759998],\n",
       "       [45.490002],\n",
       "       [46.060001],\n",
       "       [46.099998],\n",
       "       [46.34    ],\n",
       "       [47.459999],\n",
       "       [47.029999],\n",
       "       [47.110001],\n",
       "       [47.279999],\n",
       "       [46.349998],\n",
       "       [45.82    ],\n",
       "       [45.869999],\n",
       "       [45.700001],\n",
       "       [46.459999],\n",
       "       [47.240002]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2540"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) \n",
    "dataset_total.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2019, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values\n",
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89051458],\n",
       "       [0.85564501],\n",
       "       [0.86417949],\n",
       "       [0.87076325],\n",
       "       [0.86247259],\n",
       "       [0.87588393],\n",
       "       [0.88905148],\n",
       "       [0.89368447],\n",
       "       [0.90587662],\n",
       "       [0.91172887],\n",
       "       [0.92343333],\n",
       "       [0.91197269],\n",
       "       [0.91367957],\n",
       "       [0.91294809],\n",
       "       [0.92367713],\n",
       "       [0.93465009],\n",
       "       [0.94074616],\n",
       "       [0.94562304],\n",
       "       [0.98902707],\n",
       "       [0.99561083],\n",
       "       [0.98195569],\n",
       "       [1.        ],\n",
       "       [0.99097783],\n",
       "       [0.98561327],\n",
       "       [0.85003665],\n",
       "       [0.8361375 ],\n",
       "       [0.80760792],\n",
       "       [0.81004631],\n",
       "       [0.80273104],\n",
       "       [0.80955872],\n",
       "       [0.7893197 ],\n",
       "       [0.79785416],\n",
       "       [0.78810051],\n",
       "       [0.73201665],\n",
       "       [0.69641551],\n",
       "       [0.66934901],\n",
       "       [0.66569132],\n",
       "       [0.65935135],\n",
       "       [0.67276279],\n",
       "       [0.66569132],\n",
       "       [0.63691785],\n",
       "       [0.64520851],\n",
       "       [0.64057554],\n",
       "       [0.62155572],\n",
       "       [0.65130458],\n",
       "       [0.65301149],\n",
       "       [0.62106801],\n",
       "       [0.64423311],\n",
       "       [0.64130702],\n",
       "       [0.64301392],\n",
       "       [0.64081934],\n",
       "       [0.66398444],\n",
       "       [0.65520607],\n",
       "       [0.66739822],\n",
       "       [0.69129483],\n",
       "       [0.71641061],\n",
       "       [0.70202387],\n",
       "       [0.70031697],\n",
       "       [0.68154116],\n",
       "       [0.68910022],\n",
       "       [0.70397466],\n",
       "       [0.7217752 ],\n",
       "       [0.73128505],\n",
       "       [0.70885155],\n",
       "       [0.71982451],\n",
       "       [0.72372597],\n",
       "       [0.72592058],\n",
       "       [0.73811273],\n",
       "       [0.72884665],\n",
       "       [0.7659108 ],\n",
       "       [0.73226045],\n",
       "       [0.73884421],\n",
       "       [0.73518654],\n",
       "       [0.72518898],\n",
       "       [0.71884911],\n",
       "       [0.73250424],\n",
       "       [0.74811027],\n",
       "       [0.75883933],\n",
       "       [0.78322363],\n",
       "       [0.78102904],\n",
       "       [0.77103148],\n",
       "       [0.76566691],\n",
       "       [0.78224823],\n",
       "       [0.78346742],\n",
       "       [0.83199222],\n",
       "       [0.83150454],\n",
       "       [0.84442816],\n",
       "       [0.84881736],\n",
       "       [0.82053158],\n",
       "       [0.82492075],\n",
       "       [0.82004387],\n",
       "       [0.79590346],\n",
       "       [0.76103392],\n",
       "       [0.72860276],\n",
       "       [0.71372843],\n",
       "       [0.69300171],\n",
       "       [0.68958793],\n",
       "       [0.70860766],\n",
       "       [0.67983416],\n",
       "       [0.6732505 ],\n",
       "       [0.68714952],\n",
       "       [0.68812482],\n",
       "       [0.6939771 ],\n",
       "       [0.72128749],\n",
       "       [0.71080224],\n",
       "       [0.71275304],\n",
       "       [0.71689832],\n",
       "       [0.6942209 ],\n",
       "       [0.68129727],\n",
       "       [0.68251646],\n",
       "       [0.67837118],\n",
       "       [0.69690319],\n",
       "       [0.71592302]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to put them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 90 from inputs are from training set; starting\n",
    "# from 90 and get the extra 20, i.e. up to 110\n",
    "for i in range(90, 110): \n",
    "    X_test.append(inputs[i-90:i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # not 3D structure yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Intel Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Intel Stock Price')\n",
    "\n",
    "plt.title('Intel Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Intel Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "- The prediction lags behind the actual price curve because the model cannot react to fast non-linear changes. Spikes are examples of fast non-linear changes\n",
    "- Model reacts pretty well to smooth changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the RMSE\n",
    "\n",
    "If we need to compute the RMSE for our Stock Price Prediction problem, we use the real stock price and predicted stock price as shown. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7596389700507931"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:20,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
